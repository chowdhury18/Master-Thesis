{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fog CA: [0.0, 4.0, 6.0]\n",
      "Image Size: torch.Size([BATCH_SIZE, CHANNEL, ROW, COL])\n",
      "\n",
      "=> Image : 1\n",
      "torch.Size([1, 3, 227, 227])\n",
      "=> Convolution Layer: 0\n",
      "\tBefore Conv: torch.Size([1, 3, 96, 227])\n",
      "conv: Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "\tAfter Conv: torch.Size([1, 64, 11, 27])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 3, 141, 227])\n",
      "conv: Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "\tAfter Conv: torch.Size([1, 64, 16, 27])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 64, 27, 27])\n",
      "=> Convolution Layer: 1\n",
      "\tBefore Conv: torch.Size([1, 64, 13, 27])\n",
      "conv: Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "\tAfter Conv: torch.Size([1, 192, 6, 13])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 64, 18, 27])\n",
      "conv: Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "\tAfter Conv: torch.Size([1, 192, 8, 13])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 192, 14, 13])\n",
      "=> Convolution Layer: 2\n",
      "\tBefore Conv: torch.Size([1, 192, 6, 13])\n",
      "conv: Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\tAfter Conv: torch.Size([1, 384, 6, 13])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 192, 10, 13])\n",
      "conv: Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\tAfter Conv: torch.Size([1, 384, 10, 13])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 384, 16, 13])\n",
      "=> Convolution Layer: 3\n",
      "\tBefore Conv: torch.Size([1, 384, 7, 13])\n",
      "conv: Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\tAfter Conv: torch.Size([1, 256, 7, 13])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 384, 11, 13])\n",
      "conv: Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\tAfter Conv: torch.Size([1, 256, 11, 13])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 256, 18, 13])\n",
      "=> Convolution Layer: 4\n",
      "\tBefore Conv: torch.Size([1, 256, 8, 13])\n",
      "conv: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\tAfter Conv: torch.Size([1, 256, 3, 6])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 256, 12, 13])\n",
      "conv: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\tAfter Conv: torch.Size([1, 256, 5, 6])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 256, 8, 6])\n",
      "\n",
      "=> Fully Connected Layers: \n",
      "\tBefore fc: torch.Size([1, 3072])\n",
      "fc: Sequential(\n",
      "  (0): Dropout(p=0.5, inplace=False)\n",
      "  (1): Linear(in_features=3072, out_features=4096, bias=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Dropout(p=0.5, inplace=False)\n",
      "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): Linear(in_features=4096, out_features=12, bias=True)\n",
      ")\n",
      "\tAfter ff: torch.Size([1, 12])\n",
      "\n",
      "tensor([[0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]], dtype=torch.int32)\n",
      "\n",
      "\tBefore fc: torch.Size([1, 6144])\n",
      "fc: Sequential(\n",
      "  (0): Dropout(p=0.5, inplace=False)\n",
      "  (1): Linear(in_features=6144, out_features=4096, bias=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Dropout(p=0.5, inplace=False)\n",
      "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): Linear(in_features=4096, out_features=12, bias=True)\n",
      ")\n",
      "\tAfter ff: torch.Size([1, 12])\n",
      "\n",
      "tensor([[0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]], dtype=torch.int32)\n",
      "\n",
      "Final Feature Classification: tensor([[0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"module.name\", \"/home/arnab/Desktop/dnn-offloading/Models/bdddataloader.py\")\n",
    "bddloader = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(bddloader)\n",
    "\n",
    "custom_logging_format = '%(asctime)s : [%(levelname)s] - %(message)s'\n",
    "logging.basicConfig(filename= \"/home/arnab/Desktop/Data/logs/alexnet_layer_comp_time.log\" , filemode=\"a\", level= logging.INFO, format=custom_logging_format)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, padding, stride):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel, padding=padding, stride = stride)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "    def forward(self, x, weight, bias, current_layer):\n",
    "        print(\"\\tBefore Conv: {}\".format(x.size()))\n",
    "        print(f\"conv: {self.conv}\")\n",
    "        self.conv = assign_weight_bias(self.conv,weight,bias)\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        if current_layer == 0 or current_layer == 1 or current_layer == 4:\n",
    "            return self.pool(x)\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class FcBlock(nn.Module):\n",
    "    def __init__(self, in_channels,num_classes = 12):\n",
    "        super(FcBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_channels, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, model, prev_input_units):\n",
    "        print(\"\\tBefore fc: {}\".format(x.size()))\n",
    "        print(f\"fc: {self.classifier}\")\n",
    "        self.classifier = assign_weight_bias_ff(self.classifier,model,self.in_channels,prev_input_units)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def assign_weight_bias(m,weight,bias):\n",
    "    m.weight.data = weight.cpu()\n",
    "    m.bias.data = bias.cpu()\n",
    "    return m\n",
    "\n",
    "def assign_weight_bias_ff(classifier,model,input_units,prev_input_units):\n",
    "    for key,value in classifier.state_dict().items():\n",
    "        k = \"classifier.\" + str(key)\n",
    "        x = key.split(\".\")\n",
    "        if x[1] == \"weight\":\n",
    "            if x[0] == \"1\": # \"1\" to update weight size of first FF layer based on first layer input\n",
    "                weight = model[k]\n",
    "                weight = weight[:,prev_input_units:(prev_input_units + input_units)]\n",
    "                classifier[int(x[0])].weight.data = weight.cpu()\n",
    "            else:\n",
    "                classifier[int(x[0])].weight.data = model[k].cpu()\n",
    "        elif x[1] == \"bias\":\n",
    "            classifier[int(x[0])].bias.data = model[k].cpu()\n",
    "        \n",
    "    return classifier\n",
    "\n",
    "\n",
    "\n",
    "prev_input_units = 0\n",
    "in_channel = np.zeros((2),dtype=int) # worker = 2\n",
    "in_channel[0] = 3\n",
    "in_channel[1] = 3\n",
    "out_channel = [64,192,384,256,256]\n",
    "kernel_filters = [11,5,3,3,3]\n",
    "padding = [2,2,1,1,1]\n",
    "stride = [4,1,1,1,1]\n",
    "def node(NN,img_part,current_layer,BATCH_SIZE,channel,model,fog_node):\n",
    "    global prev_input_units,in_channel\n",
    "    weight = None\n",
    "    bias = None\n",
    "            \n",
    "    if NN == \"conv\":\n",
    "        c_l = current_layer * 2\n",
    "        i = 0\n",
    "        for key, value in model.items(): \n",
    "            if i == c_l:\n",
    "                weight = model[key]\n",
    "            elif i == (c_l + 1):\n",
    "                bias = model[key]\n",
    "                break\n",
    "            i += 1\n",
    "            \n",
    "        block = ConvBlock(in_channel[fog_node],out_channel[current_layer],kernel_filters[current_layer],padding[current_layer],stride[current_layer])\n",
    "        out = block(img_part,weight,bias,current_layer)\n",
    "        in_channel[fog_node] = out_channel[current_layer]\n",
    "        return out\n",
    "        \n",
    "    elif NN == \"ff\":\n",
    "        num_classes = 12\n",
    "        img_part = torch.flatten(img_part,1)\n",
    "        input_units = img_part.shape[1]\n",
    " \n",
    "        block = FcBlock(input_units)\n",
    "        out = block(img_part,model,prev_input_units)\n",
    "        prev_input_units = input_units\n",
    "        return out\n",
    "        \n",
    "    #print(m.state_dict().keys())\n",
    "\n",
    "def adaptive_partitioning(img,partition_size):\n",
    "    index = partition_size\n",
    "    temp = img.detach().numpy()\n",
    "    temp = torch.from_numpy(temp[:,:,index[0]:index[1],:])\n",
    "    return temp\n",
    "    \n",
    "def partition(NN,img,k,CA,f):\n",
    "    # intializing variables\n",
    "    A = None\n",
    "    partition_size = []\n",
    "    batch = img.shape[0]\n",
    "    channel = img.shape[1]\n",
    "    W = [0] * k  # partition\n",
    "    Windex = []  # partitioning indexes\n",
    "    init = [0] * (k + 1)\n",
    "    CA[0] = 0\n",
    "    CA = [float(val) for val in CA]\n",
    "\n",
    "    r = img.shape[2]\n",
    "    c = img.shape[3]\n",
    "    \n",
    "    if r > c:\n",
    "        m = r\n",
    "    else:\n",
    "        m = c\n",
    "    \n",
    "    \n",
    "    # sum of capabilities of all nodes\n",
    "    C2 = np.sum(CA[:(k + 1)])\n",
    "    \n",
    "    for i in range(1,k+1):\n",
    "        C1 = np.sum(CA[:i+1])\n",
    "        Pi = C1 / C2\n",
    "        if NN == \"conv\":\n",
    "            init[i] = math.floor(Pi * (m - (f - 1)))\n",
    "            partition_size.append((init[i-1],init[i]+(f-1)))\n",
    "        elif NN == \"ff\":\n",
    "            init[i] = math.floor(Pi * m)\n",
    "            partition_size.append((init[i - 1],init[i]))\n",
    "\n",
    "    return partition_size\n",
    "\n",
    "    \n",
    "def main():\n",
    "    # Load model\n",
    "    model = torch.load(\"/home/arnab/Desktop/Data/alexnet.pt\", map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Number of workers\n",
    "    worker = 2\n",
    "    \n",
    "    # Capabilities of nodes\n",
    "    #CA = np.random.randint(1, 10, size=k+1)\n",
    "    CA = [0.0, 4.0, 6.0]\n",
    "    print(\"Fog CA: {}\".format(CA))\n",
    "    \n",
    "    # Initialize variables\n",
    "    kernel_filters = [11,5,3,3,3]\n",
    "    current_layer = None\n",
    "    partition_list = {}\n",
    "    classify_list = []\n",
    "    after_part = None\n",
    "    input_ = None\n",
    "    channel = None\n",
    "    final_out = None\n",
    "    BATCH_SIZE = 1\n",
    "    IMAGE_DIM = 227\n",
    "    \n",
    "    # load images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(IMAGE_DIM),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n",
    "    \n",
    "    loader = data.DataLoader(\n",
    "        bddloader.BDDDataset('/home/arnab/Desktop/Data', train=True, transform=transform),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True)\n",
    "    \n",
    "    print(\"Image Size: torch.Size([BATCH_SIZE, CHANNEL, ROW, COL])\\n\")\n",
    "    inx = 0\n",
    "    for img,level in loader:    \n",
    "        print(\"=> Image : {}\".format(inx+1))\n",
    "        print(img.size())\n",
    "        \n",
    "        # Convolutional NN\n",
    "        for i,k in enumerate(kernel_filters):\n",
    "            current_layer = i\n",
    "            if i == 0:\n",
    "                input_ = img\n",
    "                channel = input_.shape[1]\n",
    "            else:\n",
    "                input_ = final_out\n",
    "                channel = input_.shape[1]\n",
    "                \n",
    "            key = \"Conv\" + str(i)\n",
    "            print(\"=> Convolution Layer: {}\".format(i))\n",
    "            after_part = partition(\"conv\",input_,worker,CA,f=k)\n",
    "            partition_list.update({key : after_part})\n",
    "            \n",
    "            # processing and marging\n",
    "            final_out = None\n",
    "            for j in range(len(after_part)):\n",
    "                img_part = adaptive_partitioning(input_,after_part[j])\n",
    "                out = node(\"conv\",img_part,current_layer,BATCH_SIZE,channel,model,j)\n",
    "                print(\"\\tAfter Conv: {}\\n\".format(out.size()))\n",
    "                if j == 0:\n",
    "                    final_out = out\n",
    "                else:\n",
    "                    final_out = torch.cat((final_out,out),2)\n",
    "\n",
    "            print(\"\\tAfter Marge: \" + str(final_out.size()))\n",
    "            #break\n",
    "          \n",
    "        # Adaptive average Pool\n",
    "        avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        out = avgpool(final_out)\n",
    "        \n",
    "        # Fully Connected NN\n",
    "        current_layer += 1\n",
    "        input_ = out\n",
    "        channel = input_.shape[1]\n",
    "        key = \"ff\"\n",
    "        print(\"\\n=> Fully Connected Layers: \")\n",
    "        after_part = partition(\"ff\",input_,worker,CA,f=0)\n",
    "        partition_list.update({key : after_part})\n",
    "        for j in range(len(after_part)):\n",
    "            img_part = adaptive_partitioning(input_,after_part[j])\n",
    "            out = node(\"ff\",img_part,current_layer,BATCH_SIZE,channel,model,j)\n",
    "            print(\"\\tAfter ff: {}\\n\".format(out.size()))\n",
    "            m = nn.ReLU()\n",
    "            out = m(out).data > 0\n",
    "            out = out.int()\n",
    "            classify_list.append(out)\n",
    "            print(\"{}\\n\".format(out))\n",
    "  \n",
    "        classify_final = None\n",
    "        for i in range(len(classify_list)-1):\n",
    "            if i == 0:\n",
    "                classify_final = np.bitwise_or(classify_list[i].numpy()[:], classify_list[i+1].numpy()[:]) \n",
    "            else:\n",
    "                classify_final = np.bitwise_or(classify_final,classify_list[i+1].numpy()[:])\n",
    "        \n",
    "        print(\"Final Feature Classification: {}\".format(torch.Tensor(classify_final).double()))  \n",
    "        \n",
    "        inx += 1\n",
    "        break # one image break\n",
    "        \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
