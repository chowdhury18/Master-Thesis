{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fog CA: [0.0, 4.0, 6.0]\n",
      "Input image size: torch.Size([1, 3, 32, 32])\n",
      "=> Image : 1\n",
      "=> Layers: 0\n",
      "\tBefore Conv: torch.Size([1, 3, 15, 32])\n",
      "\tConv2d(3, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "\tAfter: torch.Size([1, 192, 15, 32])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 3, 21, 32])\n",
      "\tConv2d(3, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "\tAfter: torch.Size([1, 192, 21, 32])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 192, 36, 32])\n",
      "=> Layers: 1\n",
      "\tBefore Conv: torch.Size([1, 192, 14, 32])\n",
      "\tConv2d(192, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tAfter: torch.Size([1, 160, 14, 32])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 192, 22, 32])\n",
      "\tConv2d(192, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tAfter: torch.Size([1, 160, 22, 32])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 160, 36, 32])\n",
      "=> Layers: 2\n",
      "\tBefore Conv: torch.Size([1, 160, 14, 32])\n",
      "\tConv2d(160, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tAfter: torch.Size([1, 96, 14, 32])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 160, 22, 32])\n",
      "\tConv2d(160, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tAfter: torch.Size([1, 96, 22, 32])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 96, 36, 32])\n",
      "=> Layers: 3\n",
      "\tBefore MaxPool: torch.Size([1, 96, 15, 32])\n",
      "\tMaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "\tAfter: torch.Size([1, 96, 8, 16])\n",
      "\n",
      "\tBefore MaxPool: torch.Size([1, 96, 23, 32])\n",
      "\tMaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "\tAfter: torch.Size([1, 96, 12, 16])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 96, 20, 16])\n",
      "=> Layers: 4\n",
      "\tBefore Conv: torch.Size([1, 96, 10, 16])\n",
      "\tConv2d(96, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "\tAfter: torch.Size([1, 192, 10, 16])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 96, 14, 16])\n",
      "\tConv2d(96, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "\tAfter: torch.Size([1, 192, 14, 16])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 192, 24, 16])\n",
      "=> Layers: 5\n",
      "\tBefore Conv: torch.Size([1, 192, 9, 16])\n",
      "\tConv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tAfter: torch.Size([1, 192, 9, 16])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 192, 15, 16])\n",
      "\tConv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tAfter: torch.Size([1, 192, 15, 16])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 192, 24, 16])\n",
      "=> Layers: 6\n",
      "\tBefore Conv: torch.Size([1, 192, 9, 16])\n",
      "\tConv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tAfter: torch.Size([1, 192, 9, 16])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 192, 15, 16])\n",
      "\tConv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tAfter: torch.Size([1, 192, 15, 16])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 192, 24, 16])\n",
      "=> Layers: 7\n",
      "\tBefore AvgPool: torch.Size([1, 192, 10, 16])\n",
      "\tAvgPool2d(kernel_size=3, stride=2, padding=1)\n",
      "\tAfter: torch.Size([1, 192, 5, 8])\n",
      "\n",
      "\tBefore AvgPool: torch.Size([1, 192, 16, 16])\n",
      "\tAvgPool2d(kernel_size=3, stride=2, padding=1)\n",
      "\tAfter: torch.Size([1, 192, 8, 8])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 192, 13, 8])\n",
      "=> Layers: 8\n",
      "\tBefore Conv: torch.Size([1, 192, 6, 8])\n",
      "\tConv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\tAfter: torch.Size([1, 192, 6, 8])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 192, 9, 8])\n",
      "\tConv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\tAfter: torch.Size([1, 192, 9, 8])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 192, 15, 8])\n",
      "=> Layers: 9\n",
      "\tBefore Conv: torch.Size([1, 192, 6, 8])\n",
      "\tConv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tAfter: torch.Size([1, 192, 6, 8])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 192, 9, 8])\n",
      "\tConv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tAfter: torch.Size([1, 192, 9, 8])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 192, 15, 8])\n",
      "=> Layers: 10\n",
      "\tBefore Conv: torch.Size([1, 192, 6, 8])\n",
      "\tConv2d(192, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tAfter: torch.Size([1, 12, 6, 8])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 192, 9, 8])\n",
      "\tConv2d(192, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "\tAfter: torch.Size([1, 12, 9, 8])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 12, 15, 8])\n",
      "=> Layers: 11\n",
      "\tBefore AvgPool: torch.Size([1, 12, 10, 8])\n",
      "\tAvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "\tAfter: torch.Size([1, 12, 3, 1])\n",
      "\n",
      "\tBefore AvgPool: torch.Size([1, 12, 12, 8])\n",
      "\tAvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "\tAfter: torch.Size([1, 12, 5, 1])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 12, 8, 1])\n",
      "Final output: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/jiecaoyu/pytorch-nin-cifar10\n",
    "# https://github.com/yangqiongyongyu/Network-In-Network-Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"module.name\", \"/home/arnab/Desktop/dnn-offloading/Models/bdddataloader.py\")\n",
    "bddloader = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(bddloader)\n",
    "\n",
    "in_channel = np.zeros((2),dtype=int) # worker = 2\n",
    "in_channel[0] = 3\n",
    "in_channel[1] = 3\n",
    "def node(img_part,current_layer,BATCH_SIZE,channel,model,fog_node):\n",
    "    global in_channel\n",
    "    out_channel = np.zeros((2),dtype=int) # worker = 2\n",
    "    weight = None\n",
    "    bias = None\n",
    "    layer_kernel = None\n",
    "    layer_stride = None\n",
    "    layer_padding = None\n",
    "    cfgs = [192, 160, 96, 'M', 192, 192, 192, 'A', 192, 192, 12, 'A']\n",
    "    kernel_filters = [5,1,1,3,5,1,1,3,3,1,1,8]\n",
    "    stride = [1,1,1,2,1,1,1,2,1,1,1,1]\n",
    "    padding = [2,0,0,1,2,0,0,1,1,0,0,0]\n",
    "    model_list = create_model_list(model)\n",
    "    model_dict = create_model_dict(cfgs,model_list,kernel_filters,stride,padding)\n",
    "    \n",
    "        \n",
    "    if len(model_dict[current_layer]) > 4:\n",
    "        cfg = model_dict[current_layer][0]\n",
    "        out_channel[fog_node] = model_dict[current_layer][0]\n",
    "        layer_kernel = model_dict[current_layer][1]\n",
    "        layer_stride = model_dict[current_layer][2]\n",
    "        layer_padding = model_dict[current_layer][3]\n",
    "        weight = model[model_dict[current_layer][4]]\n",
    "        bias = model[model_dict[current_layer][5]]\n",
    "    else:\n",
    "        cfg = model_dict[current_layer][0]\n",
    "        layer_kernel = model_dict[current_layer][1]\n",
    "        layer_stride = model_dict[current_layer][2]\n",
    "        layer_padding = model_dict[current_layer][3]\n",
    "        \n",
    "    dropout = nn.Dropout(0.5)\n",
    "\n",
    "    if cfg == 'M':\n",
    "        print(\"\\tBefore MaxPool: {}\".format(img_part.size()))\n",
    "        m = nn.MaxPool2d(kernel_size=layer_kernel, stride=layer_stride, padding=layer_padding)\n",
    "        print(\"\\t\" + str(m))\n",
    "        return dropout(m(img_part))\n",
    "            \n",
    "    elif cfg == 'A':\n",
    "        print(\"\\tBefore AvgPool: {}\".format(img_part.size()))\n",
    "        m = nn.AvgPool2d(kernel_size=layer_kernel, stride=layer_stride, padding=layer_padding)\n",
    "        print(\"\\t\" + str(m))\n",
    "        if current_layer != (len(cfgs)-1):\n",
    "            return dropout(m(img_part))\n",
    "        else:\n",
    "            return m(img_part)\n",
    "    else:\n",
    "        print(\"\\tBefore Conv: {}\".format(img_part.size()))\n",
    "        m0 = nn.Conv2d(in_channel[fog_node], out_channel[fog_node], kernel_size=layer_kernel, stride=layer_stride, padding=layer_padding)\n",
    "        m1 = nn.ReLU(inplace=True)\n",
    "        print(\"\\t\" + str(m0))\n",
    "        m0.weight.data = weight\n",
    "        m0.bias.data = bias\n",
    "        in_channel[fog_node] = out_channel[fog_node]\n",
    "        return m1(m0(img_part))\n",
    "        \n",
    "    #print(m.state_dict().keys())\n",
    "\n",
    "def create_model_list(model):\n",
    "    model_list = []\n",
    "    for key in model:\n",
    "        model_list.append(key)\n",
    "        \n",
    "    return model_list\n",
    "\n",
    "def create_model_dict(cfg,model_list,kernel_filters,stride,padding):\n",
    "    model_dict = {}\n",
    "    model_inx = 0\n",
    "    for i,v in enumerate(cfg):\n",
    "        if v == 'M' or v == 'A':\n",
    "            model_dict.update({i:(v,kernel_filters[i],stride[i],padding[i])})\n",
    "        else:\n",
    "            model_dict.update({i:(v,kernel_filters[i],stride[i],padding[i],model_list[model_inx],model_list[model_inx+1])})\n",
    "            model_inx += 2\n",
    "            \n",
    "    return model_dict\n",
    "\n",
    "def adaptive_partitioning(img,partition_size):\n",
    "    index = partition_size\n",
    "    temp = img.detach().numpy()\n",
    "    temp = torch.from_numpy(temp[:,:,index[0]:index[1],:])\n",
    "    return temp\n",
    "\n",
    "def partition(NN,img,k,CA,f):\n",
    "    # intializing variables\n",
    "    A = None\n",
    "    partition_size = []\n",
    "    batch = img.shape[0]\n",
    "    channel = img.shape[1]\n",
    "    W = [0] * k  # partition\n",
    "    Windex = []  # partitioning indexes\n",
    "    init = [0] * (k + 1)\n",
    "    CA[0] = 0\n",
    "    CA = [float(val) for val in CA]\n",
    "\n",
    "    r = img.shape[2]\n",
    "    c = img.shape[3]\n",
    "    \n",
    "    if r > c:\n",
    "        m = r\n",
    "    else:\n",
    "        m = c\n",
    "    \n",
    "    # sum of capabilities of all nodes\n",
    "    C2 = np.sum(CA[:(k + 1)])\n",
    "    \n",
    "    for i in range(1,k+1):\n",
    "        C1 = np.sum(CA[:i+1])\n",
    "        Pi = C1 / C2\n",
    "        if NN == \"conv\":\n",
    "            init[i] = math.floor(Pi * (m - (f - 1)))\n",
    "            partition_size.append((init[i-1],init[i]+(f-1)))\n",
    "        elif NN == \"ff\":\n",
    "            init[i] = math.floor(Pi * m)\n",
    "            partition_size.append((init[i - 1],init[i]))\n",
    "\n",
    "    return partition_size\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Configuration of network\n",
    "    cfgs = [192, 160, 96, 'M', 192, 192, 192,'A', 192, 192, 12, 'A']\n",
    "    \n",
    "    # Load model\n",
    "    model = torch.load(\"/home/arnab/Desktop/Data/nin.pt\", map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Number of workers\n",
    "    worker = 2\n",
    "    \n",
    "    # Capabilities of nodes\n",
    "    #CA = np.random.randint(1, 10, size=worker+1) \n",
    "    CA = [0.0, 4.0, 6.0] # Capabilities of nodes\n",
    "    print(\"Fog CA: {}\".format(CA))\n",
    "    \n",
    "    # Initialize variables\n",
    "    current_layer = None\n",
    "    classify_list = []\n",
    "    after_part = None\n",
    "    input_ = None\n",
    "    channel = None\n",
    "    final_out = None\n",
    "    BATCH_SIZE = 1\n",
    "    IMAGE_DIM = 32\n",
    "    kernel_filters = [5,1,1,3,5,1,1,3,3,1,1,8]\n",
    "    \n",
    "    # load images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(IMAGE_DIM),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n",
    "    \n",
    "    loader = data.DataLoader(\n",
    "        bddloader.BDDDataset('/home/arnab/Desktop/Data', train=True, transform=transform),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True)\n",
    "    \n",
    "    inx = 0\n",
    "    for img,level in loader:\n",
    "        print(f\"Input image size: {img.size()}\")\n",
    "        print(\"=> Image : {}\".format(inx+1))\n",
    "        \n",
    "        # Convolutional NN\n",
    "        for i in range(len(cfgs)):\n",
    "            current_layer = i\n",
    "            if i == 0:\n",
    "                input_ = img\n",
    "                channel = input_.shape[1]\n",
    "            else:\n",
    "                input_ = final_out\n",
    "                channel = input_.shape[1]\n",
    "\n",
    "            print(\"=> Layers: {}\".format(i))\n",
    "            after_part = partition(\"conv\",input_,worker,CA,kernel_filters[current_layer])\n",
    "\n",
    "            # processing and marging\n",
    "            final_out = None\n",
    "            for j in range(len(after_part)):\n",
    "                img_part = adaptive_partitioning(input_,after_part[j])\n",
    "                out = node(img_part,current_layer,BATCH_SIZE,channel,model,j)\n",
    "                print(\"\\tAfter: {}\\n\".format(out.size()))\n",
    "                if j == 0:\n",
    "                    final_out = out\n",
    "                else:\n",
    "                    final_out = torch.cat((final_out,out),2)\n",
    "                    \n",
    "            print(\"\\tAfter Marge: \" + str(final_out.size()))\n",
    "        \n",
    "        avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        final_out = avgpool(final_out)\n",
    "        #final_out = final_out.view(final_out.size(0), 12)\n",
    "        final_out = torch.flatten(final_out,1)\n",
    "        m = nn.ReLU()\n",
    "        out = m(final_out).data > 0\n",
    "        out = out.int()\n",
    "        print(f\"Final output: {out.size()}\")\n",
    "        \n",
    "        inx += 1\n",
    "        break # image break\n",
    "    \n",
    "    out.size()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
