{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fog CA: [0.0, 4.0, 6.0]\n",
      "Image Size: torch.Size([BATCH_SIZE, CHANNEL, ROW, COL])\n",
      "\n",
      "=> Image : 1\n",
      "torch.Size([1, 3, 224, 224])\n",
      "=> Convolution Layers: 0\n",
      "\tBefore Conv: torch.Size([1, 3, 93, 224])\n",
      "First Block: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "\tAfter: torch.Size([1, 64, 24, 56])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 3, 137, 224])\n",
      "First Block: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "\tAfter: torch.Size([1, 64, 35, 56])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 64, 59, 56])\n",
      "=> Convolution Layers: 1\n",
      "\tBefore Conv: torch.Size([1, 64, 24, 56])\n",
      "Block: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 64, 24, 56])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 64, 37, 56])\n",
      "Block: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 64, 37, 56])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 64, 61, 56])\n",
      "=> Convolution Layers: 2\n",
      "\tBefore Conv: torch.Size([1, 64, 25, 56])\n",
      "Block: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 64, 25, 56])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 64, 38, 56])\n",
      "Block: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 64, 38, 56])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 64, 63, 56])\n",
      "=> Convolution Layers: 3\n",
      "\tBefore Conv: torch.Size([1, 64, 26, 56])\n",
      "Block: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 64, 26, 56])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 64, 39, 56])\n",
      "Block: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 64, 39, 56])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 64, 65, 56])\n",
      "=> Convolution Layers: 4\n",
      "\tBefore Conv: torch.Size([1, 64, 27, 56])\n",
      "Block: Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "\tAfter: torch.Size([1, 128, 14, 28])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 64, 40, 56])\n",
      "Block: Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "\tAfter: torch.Size([1, 128, 20, 28])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 128, 34, 28])\n",
      "=> Convolution Layers: 5\n",
      "\tBefore Conv: torch.Size([1, 128, 14, 28])\n",
      "Block: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 128, 14, 28])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 128, 22, 28])\n",
      "Block: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 128, 22, 28])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 128, 36, 28])\n",
      "=> Convolution Layers: 6\n",
      "\tBefore Conv: torch.Size([1, 128, 15, 28])\n",
      "Block: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 128, 15, 28])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 128, 23, 28])\n",
      "Block: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 128, 23, 28])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 128, 38, 28])\n",
      "=> Convolution Layers: 7\n",
      "\tBefore Conv: torch.Size([1, 128, 16, 28])\n",
      "Block: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 128, 16, 28])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 128, 24, 28])\n",
      "Block: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 128, 24, 28])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 128, 40, 28])\n",
      "=> Convolution Layers: 8\n",
      "\tBefore Conv: torch.Size([1, 128, 17, 28])\n",
      "Block: Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "\tAfter: torch.Size([1, 256, 9, 14])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 128, 25, 28])\n",
      "Block: Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "\tAfter: torch.Size([1, 256, 13, 14])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 256, 22, 14])\n",
      "=> Convolution Layers: 9\n",
      "\tBefore Conv: torch.Size([1, 256, 10, 14])\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 256, 10, 14])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 256, 14, 14])\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 256, 14, 14])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 256, 24, 14])\n",
      "=> Convolution Layers: 10\n",
      "\tBefore Conv: torch.Size([1, 256, 10, 14])\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 256, 10, 14])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 256, 16, 14])\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 256, 16, 14])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 256, 26, 14])\n",
      "=> Convolution Layers: 11\n",
      "\tBefore Conv: torch.Size([1, 256, 11, 14])\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 256, 11, 14])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 256, 17, 14])\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 256, 17, 14])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 256, 28, 14])\n",
      "=> Convolution Layers: 12\n",
      "\tBefore Conv: torch.Size([1, 256, 12, 14])\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 256, 12, 14])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 256, 18, 14])\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 256, 18, 14])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 256, 30, 14])\n",
      "=> Convolution Layers: 13\n",
      "\tBefore Conv: torch.Size([1, 256, 13, 14])\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 256, 13, 14])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 256, 19, 14])\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 256, 19, 14])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 256, 32, 14])\n",
      "=> Convolution Layers: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBefore Conv: torch.Size([1, 256, 14, 14])\n",
      "Block: Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "\tAfter: torch.Size([1, 512, 7, 7])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 256, 20, 14])\n",
      "Block: Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "\tAfter: torch.Size([1, 512, 10, 7])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 512, 17, 7])\n",
      "=> Convolution Layers: 15\n",
      "\tBefore Conv: torch.Size([1, 512, 8, 7])\n",
      "Block: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 512, 8, 7])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 512, 11, 7])\n",
      "Block: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 512, 11, 7])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 512, 19, 7])\n",
      "=> Convolution Layers: 16\n",
      "\tBefore Conv: torch.Size([1, 512, 8, 7])\n",
      "Block: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 512, 8, 7])\n",
      "\n",
      "\tBefore Conv: torch.Size([1, 512, 13, 7])\n",
      "Block: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Block: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\tAfter: torch.Size([1, 512, 13, 7])\n",
      "\n",
      "\tAfter Marge: torch.Size([1, 512, 21, 7])\n",
      "torch.Size([1, 512, 1, 1])\n",
      "=> Fully Connected Layers: \n",
      "\tBefore ff: torch.Size([1, 512])\n",
      "FC: Linear(in_features=512, out_features=12, bias=True)\n",
      "\tAfter ff: torch.Size([1, 12])\n",
      "\n",
      "tensor([[0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]], dtype=torch.int32)\n",
      "\n",
      "\tBefore ff: torch.Size([1, 512])\n",
      "FC: Linear(in_features=512, out_features=12, bias=True)\n",
      "\tAfter ff: torch.Size([1, 12])\n",
      "\n",
      "tensor([[0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]], dtype=torch.int32)\n",
      "\n",
      "Final Feature Classification: tensor([[0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"module.name\", \"/home/arnab/Desktop/dnn-offloading/Models/bdddataloader.py\")\n",
    "bddloader = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(bddloader)\n",
    "\n",
    "\n",
    "def first_block(img_part,model,model_parameters):\n",
    "    print(\"\\tBefore Conv: {}\".format(img_part.size()))\n",
    "    conv1 = nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3, bias = False)\n",
    "    print(f\"First Block: {conv1}\")\n",
    "    bn1 = nn.BatchNorm2d(64) \n",
    "    conv1,bn1 = assign_model_conv([conv1,bn1],0,model,model_parameters)\n",
    "    activation1 = nn.ReLU(inplace = True)\n",
    "    pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding = 1)\n",
    "    return pool1(activation1(bn1(conv1(img_part))))\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1):\n",
    "        super(Block, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias = False, stride = stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size = 1, bias = False, stride = stride)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x, model, model_parameters):\n",
    "        print(\"\\tBefore Conv: {}\".format(x.size()))\n",
    "        self.conv1,self.bn1 = assign_model_conv([self.conv1,self.bn1],0,model,model_parameters)\n",
    "        self.conv2,self.bn2 = assign_model_conv([self.conv2,self.bn2],6,model,model_parameters)\n",
    "        print(f\"Block: {self.conv1}\")\n",
    "        print(f\"Block: {self.conv2}\")\n",
    "        \n",
    "        out1 = self.conv1(x)\n",
    "        out1 = self.bn1(out1)\n",
    "        out1 = self.activation(out1)\n",
    "        \n",
    "        out1 = self.conv2(out1)\n",
    "        out1 = self.bn2(out1)\n",
    "        if self.in_channels != self.out_channels or self.stride > 1:\n",
    "            self.conv3,self.bn3 = assign_model_conv([self.conv3,self.bn3],12,model,model_parameters)\n",
    "            print(f\"Block: {self.conv3}\")\n",
    "            out2 = self.conv3(x)\n",
    "            out2 = self.bn3(out2)\n",
    "            return self.activation(out1+out2)\n",
    "        else:\n",
    "            return self.activation(out1+x)\n",
    "        \n",
    "def assign_model_conv(conv_layers,current_index,model,model_parameters):\n",
    "    conv_layers[0].weight.data = model[model_parameters[current_index]]\n",
    "    conv_layers[1].weight.data = model[model_parameters[current_index+1]]\n",
    "    conv_layers[1].bias.data = model[model_parameters[current_index+2]]\n",
    "    conv_layers[1].running_mean.data = model[model_parameters[current_index+3]]\n",
    "    conv_layers[1].running_var.data = model[model_parameters[current_index+4]]\n",
    "    conv_layers[1].num_batches_tracked.data = model[model_parameters[current_index+5]]\n",
    "    return conv_layers[0],conv_layers[1]\n",
    "\n",
    "def assign_model_fc(fc,model,input_units,prev_input_units):\n",
    "    fc.weight.data = model['fc.weight'][:,prev_input_units: (prev_input_units + input_units)]\n",
    "    fc.bias.data = model['fc.bias']\n",
    "    return fc\n",
    "\n",
    "def create_model_list(model):\n",
    "    model_list = []\n",
    "    for key in model:\n",
    "        model_list.append(key)        \n",
    "    return model_list\n",
    "\n",
    "def create_model_dict(cfgs,model_list):\n",
    "    model_dict = {}\n",
    "    temp = []\n",
    "    current_model_list_inx = 0\n",
    "    for i,v in enumerate(cfgs):\n",
    "        if len(v) == 2:\n",
    "            out_channel = v[0]\n",
    "            num_model_parameters = v[1]\n",
    "        elif len(v) == 3:\n",
    "            out_channel = v[0]\n",
    "            num_model_parameters = v[1]\n",
    "            stride = v[2]\n",
    "            \n",
    "        for j in range(current_model_list_inx,current_model_list_inx+num_model_parameters):\n",
    "            temp.append(model_list[j])\n",
    "        model_dict.update({i:temp})\n",
    "        temp = []\n",
    "        current_model_list_inx += num_model_parameters\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "in_channel = np.zeros((2),dtype=int) # worker = 2\n",
    "in_channel[0] = 3\n",
    "in_channel[1] = 3\n",
    "prev_input_units = 0\n",
    "def node(NN,img_part,current_layer,BATCH_SIZE,channel,model,fog_node):\n",
    "    global prev_input_units,in_channel\n",
    "    out_channel = np.zeros((2),dtype=int) # worker = 2\n",
    "    weight = None\n",
    "    bias = None\n",
    "    cfgs = [(64,6), (64,12), (64,12), (64,12), (128,18,2), (128,12), (128,12), (128,12), (256,18,2), (256,12), (256,12), (256,12), (256,12), (256,12), (512,18,2), (512,12), (512,12)]\n",
    "    model_list = create_model_list(model)\n",
    "    model_dict = create_model_dict(cfgs,model_list)\n",
    "    \n",
    "    if NN == \"conv\":\n",
    "        if len(cfgs[current_layer]) == 2:\n",
    "            out_channel[fog_node] = cfgs[current_layer][0]\n",
    "            in_channel[fog_node] = out_channel[fog_node]\n",
    "            stride = 1\n",
    "        elif len(cfgs[current_layer]) == 3:\n",
    "            out_channel[fog_node] = cfgs[current_layer][0]\n",
    "            stride = cfgs[current_layer][2]\n",
    "            \n",
    "        model_parameters = model_dict[current_layer]\n",
    "        if current_layer == 0:\n",
    "            out = first_block(img_part,model,model_parameters)\n",
    "            return out\n",
    "        else:\n",
    "            block = Block(in_channels = in_channel[fog_node], out_channels = out_channel[fog_node], stride = stride)\n",
    "            out = block(img_part,model,model_parameters)\n",
    "            return out\n",
    "    elif NN == \"ff\":\n",
    "        num_classes = 12\n",
    "        img_part = torch.flatten(img_part,1)\n",
    "        \n",
    "        print(\"\\tBefore ff: {}\".format(img_part.size()))\n",
    "        fc = nn.Linear(512, num_classes)\n",
    "        print(f\"FC: {fc}\")\n",
    "        fc.weight.data = model['fc.weight']\n",
    "        fc.bias.data = model['fc.bias']\n",
    "        return fc(img_part)\n",
    "\n",
    "def adaptive_partitioning(img,partition_size):\n",
    "    index = partition_size\n",
    "    temp = img.detach().numpy()\n",
    "    temp = torch.from_numpy(temp[:,:,index[0]:index[1],:])\n",
    "    return temp\n",
    "\n",
    "def partition(NN,img,k,CA,f):\n",
    "    # intializing variables\n",
    "    A = None\n",
    "    partition_size = []\n",
    "    batch = img.shape[0]\n",
    "    channel = img.shape[1]\n",
    "    W = [0] * k  # partition\n",
    "    Windex = []  # partitioning indexes\n",
    "    init = [0] * (k + 1)\n",
    "    CA[0] = 0\n",
    "    CA = [float(val) for val in CA]\n",
    "\n",
    "    r = img.shape[2]\n",
    "    c = img.shape[3]\n",
    "    \n",
    "    if r > c:\n",
    "        m = r\n",
    "    else:\n",
    "        m = c\n",
    "    \n",
    "    \n",
    "    # sum of capabilities of all nodes\n",
    "    C2 = np.sum(CA[:(k + 1)])\n",
    "    \n",
    "    for i in range(1,k+1):\n",
    "        C1 = np.sum(CA[:i+1])\n",
    "        Pi = C1 / C2\n",
    "        if NN == \"conv\":\n",
    "            init[i] = math.floor(Pi * (m - (f - 1)))\n",
    "            partition_size.append((init[i-1],init[i]+(f-1)))\n",
    "        elif NN == \"ff\":\n",
    "            init[i] = math.floor(Pi * m)\n",
    "            partition_size.append((init[i - 1],init[i]))\n",
    "\n",
    "    return partition_size\n",
    "\n",
    "def main():\n",
    "    # Load model\n",
    "    model = torch.load(\"/home/arnab/Desktop/Data/resnet34.pt\", map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Configuration of network\n",
    "    cfgs = [(64,6), (64,12), (64,12), (64,12), (128,18,2), (128,12), (128,12), (128,12), (256,18,2), (256,12), (256,12), (256,12), (256,12), (256,12), (512,18,2), (512,12), (512,12)]\n",
    "    \n",
    "    # Number of workers\n",
    "    worker = 2\n",
    "    \n",
    "    # Capabilities of nodes\n",
    "    #CA = np.random.randint(1, 10, size=k+1)\n",
    "    CA = [0.0, 4.0, 6.0]\n",
    "    print(\"Fog CA: {}\".format(CA))\n",
    "    \n",
    "    # Initialize variables\n",
    "    kernel_filters = None\n",
    "    current_layer = None\n",
    "    partition_list = {}\n",
    "    classify_list = []\n",
    "    after_part = None\n",
    "    input_ = None\n",
    "    channel = None\n",
    "    final_out = None\n",
    "    BATCH_SIZE = 1\n",
    "    IMAGE_DIM = 224\n",
    "    \n",
    "    # load images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(IMAGE_DIM),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n",
    "    \n",
    "    loader = data.DataLoader(\n",
    "        bddloader.BDDDataset('/home/arnab/Desktop/Data', train=True, transform=transform),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True)\n",
    "    \n",
    "    print(\"Image Size: torch.Size([BATCH_SIZE, CHANNEL, ROW, COL])\\n\")\n",
    "    inx = 0\n",
    "    for img,level in loader:\n",
    "        print(\"=> Image : {}\".format(inx+1))\n",
    "        print(img.size())\n",
    "\n",
    "        # Convolutional NN\n",
    "        for i in range(len(cfgs)):\n",
    "            current_layer = i\n",
    "            if i == 0:\n",
    "                input_ = img\n",
    "                channel = input_.shape[1]\n",
    "                kernel_filters = 7\n",
    "            else:\n",
    "                input_ = final_out\n",
    "                channel = input_.shape[1]\n",
    "                kernel_filters = 3\n",
    "\n",
    "            key = \"Conv\" + str(i)\n",
    "            print(\"=> Convolution Layers: {}\".format(i))\n",
    "            after_part = partition(\"conv\",input_,worker,CA,kernel_filters)\n",
    "            partition_list.update({key : after_part})\n",
    "\n",
    "            # processing and marging\n",
    "            final_out = None\n",
    "            for j in range(len(after_part)):\n",
    "                img_part = adaptive_partitioning(input_,after_part[j])\n",
    "                out = node(\"conv\",img_part,current_layer,BATCH_SIZE,channel,model,j)\n",
    "                print(\"\\tAfter: {}\\n\".format(out.size()))\n",
    "                if j == 0:\n",
    "                    final_out = out\n",
    "                else:\n",
    "                    final_out = torch.cat((final_out,out),2)\n",
    "\n",
    "            print(\"\\tAfter Marge: \" + str(final_out.size()))\n",
    "            \n",
    "        \n",
    "        \n",
    "        # Adaptive average Pool\n",
    "        avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        out = avgpool(final_out)\n",
    "        print(out.size())\n",
    "        \n",
    "        # Fully Connected NN\n",
    "        current_layer += 1\n",
    "        input_ = out\n",
    "        channel = input_.shape[1]\n",
    "        key = \"ff\"\n",
    "        print(\"=> Fully Connected Layers: \")\n",
    "        for j in range(worker):\n",
    "            img_part = input_\n",
    "            out = node(\"ff\",img_part,current_layer,BATCH_SIZE,channel,model,j)\n",
    "            print(\"\\tAfter ff: {}\\n\".format(out.size()))\n",
    "            m = nn.ReLU()\n",
    "            out = m(out).data > 0\n",
    "            out = out.int()\n",
    "            classify_list.append(out)\n",
    "            print(\"{}\\n\".format(out))\n",
    "        \n",
    "        \n",
    "        classify_final = None\n",
    "        for i in range(len(classify_list)-1):\n",
    "            if i == 0:\n",
    "                classify_final = np.bitwise_or(classify_list[i].numpy()[:], classify_list[i+1].numpy()[:]) \n",
    "            else:\n",
    "                classify_final = np.bitwise_or(classify_final,classify_list[i+1].numpy()[:])\n",
    "\n",
    "        print(\"Final Feature Classification: {}\".format(torch.Tensor(classify_final).double()))\n",
    "        \n",
    "        inx += 1\n",
    "        break # one image break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
